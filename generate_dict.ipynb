{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=pd.read_csv(\"russian3/words.csv\")\n",
    "print(words.columns)\n",
    "words[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=pd.concat([words[:100], words[words[\"id\"]==59737]])\n",
    "print(words[\"type\"].unique())\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nan_list=words[~pd.isna(words[\"type\"])]\n",
    "print(\"Total Not NaN:\", len(not_nan_list))\n",
    "print(\"Total Not NaN (not disabled):\", len(not_nan_list[not_nan_list[\"disabled\"]==0]))\n",
    "print(\"Total Not NaN (disabled):\", len(not_nan_list[not_nan_list[\"disabled\"]==1]))\n",
    "not_nan_list=not_nan_list[not_nan_list[\"disabled\"]==1]\n",
    "print(\"Has Usage (disabled):\", len(not_nan_list[not_nan_list[\"usage_en\"].isna()==False]))\n",
    "del not_nan_list\n",
    "\n",
    "print()\n",
    "nan_list=words[pd.isna(words[\"type\"])]\n",
    "print(\"Total NaN:\", len(nan_list))\n",
    "print(\"Total NaN (not disabled):\", len(nan_list[nan_list[\"disabled\"]==0]))\n",
    "print(\"Total NaN (disabled):\", len(nan_list[nan_list[\"disabled\"]==1]))\n",
    "nan_list=nan_list[nan_list[\"disabled\"]==0]\n",
    "print(\"Has Usage (not disabled):\", len(nan_list[nan_list[\"usage_en\"].isna()==False]))\n",
    "del nan_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabled的词、type为NaN的词，将被剔除\n",
    "words=words[~pd.isna(words[\"type\"])]\n",
    "words=words[words[\"disabled\"]==0]\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_csv=pd.read_csv(\"russian3/nouns.csv\")\n",
    "words_forms_csv=pd.read_csv(\"russian3/words_forms.csv\")\n",
    "translations_csv=pd.read_csv(\"russian3/translations.csv\")\n",
    "expressions_words_csv=pd.read_csv(\"russian3/expressions_words.csv\")\n",
    "\n",
    "# 剔除德语的翻译\n",
    "print(len(translations_csv))\n",
    "translations_csv=translations_csv[translations_csv[\"lang\"]==\"en\"]\n",
    "print(len(translations_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accented(derived_from_word_id: int):\n",
    "    return convertStress(words[words[\"id\"]==derived_from_word_id].iloc[0][\"accented\"])\n",
    "\n",
    "def get_translations(word_id: int):\n",
    "    table=translations_csv[translations_csv[\"word_id\"]==word_id]\n",
    "    translation_list=[]\n",
    "    for i, row in table.iterrows():\n",
    "        translation_list.append([\n",
    "            row[\"tl\"] if not pd.isna(row[\"tl\"]) else \"\",\n",
    "            convertStress(row[\"example_ru\"]) if not pd.isna(row[\"example_ru\"]) else \"\",\n",
    "            row[\"example_tl\"] if not pd.isna(row[\"example_tl\"]) else \"\",\n",
    "            row[\"info\"] if not pd.isna(row[\"info\"]) else \"\",\n",
    "        ])\n",
    "    return translation_list\n",
    "\n",
    "def get_expressions(word_id: int):\n",
    "    expression_id_list=expressions_words_csv[expressions_words_csv[\"referenced_word_id\"]==word_id][\"expression_id\"].values.tolist()\n",
    "    expression_list=[]\n",
    "    for expression_id in expression_id_list:\n",
    "        expression_list.append([\n",
    "            get_accented(expression_id),\n",
    "            \"; \".join([i[0] for i in get_translations(expression_id)])\n",
    "        ])\n",
    "    return expression_list\n",
    "\n",
    "def get_sentences(word_id: int):\n",
    "    pass\n",
    "\n",
    "def get_relateds(word_id: int):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_get_info(word_id: int):\n",
    "    row=nouns_csv[nouns_csv[\"word_id\"]==word_id].iloc[0]\n",
    "    info={\n",
    "        \"gender\": row[\"gender\"],\n",
    "        \"partner\": convertStress(row[\"partner\"]) if not pd.isna(row[\"partner\"]) else \"\",\n",
    "        \"indeclinable\": True if row[\"indeclinable\"] else False,\n",
    "        \"animate\": True if row[\"animate\"] else False,\n",
    "        \"sg_only\": True if row[\"sg_only\"] else False,\n",
    "        \"pl_only\": True if row[\"pl_only\"] else False,\n",
    "    }\n",
    "    return info\n",
    "\n",
    "def noun_get_declension(word_id: int):\n",
    "    table=words_forms_csv[words_forms_csv[\"word_id\"]==word_id]\n",
    "    declension_list=[\"ru_noun_sg_nom\",\"ru_noun_sg_gen\",\"ru_noun_sg_dat\",\"ru_noun_sg_acc\",\"ru_noun_sg_inst\",\"ru_noun_sg_prep\",\"ru_noun_pl_nom\",\"ru_noun_pl_gen\",\"ru_noun_pl_dat\",\"ru_noun_pl_acc\",\"ru_noun_pl_inst\",\"ru_noun_pl_prep\"]\n",
    "    declension_dict={}\n",
    "    for dec in declension_list:\n",
    "        form=\", \".join(table[table[\"form_type\"]==dec][\"form\"])\n",
    "        # form_bare=\", \".join(table[table[\"form_type\"]==dec][\"form_bare\"])\n",
    "        declension_dict[dec]=convertStress(form)\n",
    "    return declension_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={}\n",
    "for i,row in words.iterrows():\n",
    "    word_id=row[\"id\"]\n",
    "    bare=row[\"bare\"]\n",
    "    accented=row[\"accented\"]\n",
    "    derived_from_word_id=row[\"derived_from_word_id\"]\n",
    "    rank=row[\"rank\"]\n",
    "    usage_en=row[\"usage_en\"]\n",
    "    Type=row[\"type\"]\n",
    "    \n",
    "    if word_dict.get(bare)==None:\n",
    "        word_dict[bare]=[]\n",
    "    \n",
    "    temp_dict={\n",
    "        \"id\": word_id,\n",
    "        \"overview\":{\n",
    "            \"Type\": Type,\n",
    "            \"accented\": accented,\n",
    "            \"derived_from_word\": get_accented(derived_from_word_id),\n",
    "            \"rank\": rank\n",
    "        },\n",
    "        \"translations\": get_translations(word_id),\n",
    "        \"usage\": usage_en,\n",
    "        \"expressions\": get_expressions(word_id),\n",
    "        \"sentences\": get_sentences(word_id),\n",
    "        \"relateds\": get_relateds(word_id),\n",
    "    }\n",
    "\n",
    "    if Type==\"noun\":\n",
    "        temp_dict[\"overview\"][\"extra\"]=noun_get_info(word_id)\n",
    "        temp_dict[\"declension\"]=noun_get_declension(word_id)\n",
    "    \n",
    "    word_dict[bare].append(temp_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
