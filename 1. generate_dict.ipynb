{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import *\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def show_na_column(df):\n",
    "    print(\"NaN:\", [i for i in list(df.isnull().sum().items()) if i[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=pd.read_csv(\n",
    "    \"russian3/words.csv\",\n",
    "    usecols=[\"id\", \"bare\", \"accented\", \"derived_from_word_id\", \"rank\", \"disabled\", \"usage_en\", \"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有些词竟然还有多余的空格……\n",
    "print(words[\"bare\"].str.contains(\" $\").sum())\n",
    "print(words[\"bare\"].str.contains(\"^ \").sum())\n",
    "words[\"bare\"]=words[\"bare\"].apply(lambda x:x.strip())\n",
    "print(words[\"bare\"].str.contains(\" $\").sum())\n",
    "print(words[\"bare\"].str.contains(\"^ \").sum())\n",
    "\n",
    "print(words[\"accented\"].str.contains(\" $\").sum())\n",
    "print(words[\"accented\"].str.contains(\"^ \").sum())\n",
    "words[\"accented\"]=words[\"accented\"].apply(lambda x:x.strip())\n",
    "print(words[\"accented\"].str.contains(\" $\").sum())\n",
    "print(words[\"accented\"].str.contains(\"^ \").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"derived_from_word_id\"].fillna(-1, inplace=True)\n",
    "words[\"rank\"].fillna(-1, inplace=True)\n",
    "words[\"usage_en\"].fillna(\"\", inplace=True)\n",
    "words[\"usage_en\"].replace(\"\\\\\\\\n\", \"\\\\n\", regex=True,inplace=True)\n",
    "dtype={\"id\":\"int\", \"bare\":\"string\", \"accented\":\"string\", \"derived_from_word_id\":\"int\", \"rank\":\"int\", \"disabled\":\"int\", \"usage_en\":\"string\", \"type\":\"string\"}\n",
    "words=words.astype(dtype)\n",
    "words.info()\n",
    "show_na_column(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nan_list=words[~pd.isna(words[\"type\"])]\n",
    "print(\"Total Not NaN:\", len(not_nan_list))\n",
    "print(\"Total Not NaN (not disabled):\", len(not_nan_list[not_nan_list[\"disabled\"]==0]))\n",
    "print(\"Total Not NaN (disabled):\", len(not_nan_list[not_nan_list[\"disabled\"]==1]))\n",
    "not_nan_list=not_nan_list[not_nan_list[\"disabled\"]==1]\n",
    "print(\"Has Usage (disabled):\", len(not_nan_list[not_nan_list[\"usage_en\"].isna()==False]))\n",
    "del not_nan_list\n",
    "\n",
    "print()\n",
    "nan_list=words[pd.isna(words[\"type\"])]\n",
    "print(\"Total NaN:\", len(nan_list))\n",
    "print(\"Total NaN (not disabled):\", len(nan_list[nan_list[\"disabled\"]==0]))\n",
    "print(\"Total NaN (disabled):\", len(nan_list[nan_list[\"disabled\"]==1]))\n",
    "nan_list=nan_list[nan_list[\"disabled\"]==0]\n",
    "print(\"Has Usage (not disabled):\", len(nan_list[nan_list[\"usage_en\"].isna()==False]))\n",
    "del nan_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabled的词、type为NaN的词，将没有主页面，但是可以被relate到\n",
    "selected_words=words[~pd.isna(words[\"type\"])].copy(deep=True)\n",
    "selected_words=selected_words[selected_words[\"disabled\"]==0]\n",
    "selected_words.drop(columns=[\"disabled\"], inplace=True)\n",
    "selected_words.info()\n",
    "show_na_column(selected_words)\n",
    "\n",
    "other_words=words[(pd.isna(words[\"type\"])) | (words[\"disabled\"]==1)].copy(deep=True)\n",
    "other_words.drop(columns=[\"bare\", \"derived_from_word_id\", \"rank\", \"disabled\", \"usage_en\", \"type\"], inplace=True)\n",
    "other_words.info()\n",
    "show_na_column(other_words)\n",
    "\n",
    "del words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_forms_csv=pd.read_csv(\"russian3/words_forms.csv\", usecols=[\"word_id\",\"form_type\",\"form\"])\n",
    "words_forms_csv[\"form\"].fillna(\"\", inplace=True)\n",
    "\n",
    "# 有些词竟然还有多余的空格……\n",
    "print(words_forms_csv[\"form\"].str.contains(\" $\").sum())\n",
    "print(words_forms_csv[\"form\"].str.contains(\"^ \").sum())\n",
    "words_forms_csv[\"form\"]=words_forms_csv[\"form\"].apply(lambda x:x.strip())\n",
    "print(words_forms_csv[\"form\"].str.contains(\" $\").sum())\n",
    "print(words_forms_csv[\"form\"].str.contains(\"^ \").sum())\n",
    "\n",
    "dtype={\"word_id\":\"int\", \"form_type\":\"string\", \"form\":\"string\"}\n",
    "words_forms_csv=words_forms_csv.astype(dtype)\n",
    "words_forms_csv.info(show_counts=True)\n",
    "show_na_column(words_forms_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rels_csv=pd.read_csv(\"russian3/words_rels.csv\", usecols=[\"word_id\",\"rel_word_id\",\"relation\"])\n",
    "dtype={\"word_id\":\"int\", \"rel_word_id\":\"int\", \"relation\":\"string\"}\n",
    "words_rels_csv=words_rels_csv.astype(dtype)\n",
    "words_rels_csv.info()\n",
    "show_na_column(words_rels_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_csv=pd.read_csv(\"russian3/nouns.csv\")\n",
    "# both->b\n",
    "nouns_csv[\"gender\"]=nouns_csv[\"gender\"].map({\"f\":\"f\", \"m\":\"m\", \"n\":\"n\", \"pl\":\"pl\",\"both\":\"b\"})\n",
    "nouns_csv[\"gender\"].fillna(\"\", inplace=True)\n",
    "nouns_csv[\"partner\"].fillna(\"\", inplace=True)\n",
    "nouns_csv[\"animate\"].fillna(0, inplace=True)\n",
    "nouns_csv[\"indeclinable\"].fillna(0, inplace=True)\n",
    "nouns_csv[\"sg_only\"].fillna(0, inplace=True)\n",
    "nouns_csv[\"pl_only\"].fillna(0, inplace=True)\n",
    "dtype={\"word_id\":\"int\", \"gender\":\"string\", \"partner\":\"string\", \"animate\":\"bool\", \"indeclinable\":\"bool\", \"sg_only\":\"bool\", \"pl_only\":\"bool\"}\n",
    "nouns_csv=nouns_csv.astype(dtype)\n",
    "nouns_csv.info()\n",
    "show_na_column(nouns_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_csv=pd.read_csv(\"russian3/verbs.csv\", usecols=[\"word_id\",\"aspect\",\"partner\"])\n",
    "# imperfective->i, perfective->p, both->b\n",
    "verbs_csv[\"aspect\"]=verbs_csv[\"aspect\"].map({\"imperfective\":\"i\", \"perfective\":\"p\", \"both\":\"b\"})\n",
    "verbs_csv[\"aspect\"].fillna(\"\", inplace=True)\n",
    "verbs_csv[\"partner\"].fillna(\"\", inplace=True)\n",
    "dtype={\"word_id\":\"int\", \"aspect\":\"string\", \"partner\":\"string\"}\n",
    "verbs_csv=verbs_csv.astype(dtype)\n",
    "verbs_csv.info()\n",
    "show_na_column(verbs_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expressions_words_csv=pd.read_csv(\"russian3/expressions_words.csv\", usecols=[\"expression_id\", \"referenced_word_id\"])\n",
    "dtype={\"expression_id\":\"int\", \"referenced_word_id\":\"int\"}\n",
    "expressions_words_csv=expressions_words_csv.astype(dtype)\n",
    "expressions_words_csv.info()\n",
    "show_na_column(expressions_words_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations_csv=pd.read_csv(\"russian3/translations.csv\")\n",
    "translations_csv=translations_csv[translations_csv[\"lang\"]==\"en\"] # 只留英语的翻译\n",
    "translations_csv.drop(columns=[\"id\", \"lang\", \"position\"], inplace=True)\n",
    "translations_csv[\"example_ru\"].fillna(\"\", inplace=True)\n",
    "translations_csv[\"example_tl\"].fillna(\"\", inplace=True)\n",
    "translations_csv[\"info\"].fillna(\"\", inplace=True)\n",
    "dtype={\"word_id\":\"int\", \"tl\":\"string\", \"example_ru\":\"string\", \"example_tl\":\"string\", \"info\":\"string\"}\n",
    "translations_csv=translations_csv.astype(dtype)\n",
    "translations_csv.info()\n",
    "show_na_column(translations_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_csv=pd.read_csv(\"russian3/sentences.csv\", usecols=[\"id\", \"ru\"])\n",
    "dtype={\"id\":\"int\", \"ru\":\"string\"}\n",
    "sentences_csv=sentences_csv.astype(dtype)\n",
    "sentences_csv.info()\n",
    "show_na_column(sentences_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_words_csv=pd.read_csv(\"russian3/sentences_words.csv\", usecols=[\"sentence_id\", \"word_id\"])\n",
    "dtype={\"sentence_id\":\"int\", \"word_id\":\"int\"}\n",
    "sentences_words_csv=sentences_words_csv.astype(dtype)\n",
    "sentences_words_csv.info()\n",
    "show_na_column(sentences_words_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_translations_csv=pd.read_csv(\"russian3/sentences_translations.csv\", usecols=[\"sentence_id\", \"tl_en\"])\n",
    "sentences_translations_csv=sentences_translations_csv[sentences_translations_csv[\"tl_en\"].isna()==False]\n",
    "dtype={\"sentence_id\":\"int\", \"tl_en\":\"string\"}\n",
    "sentences_translations_csv=sentences_translations_csv.astype(dtype)\n",
    "sentences_translations_csv.info()\n",
    "show_na_column(sentences_translations_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accented(word_id: int):\n",
    "    accented=\"\"\n",
    "    if word_id in selected_words[\"id\"].values:\n",
    "        accented=convertStress(selected_words[selected_words[\"id\"]==word_id].iloc[0][\"accented\"])\n",
    "    elif word_id in other_words[\"id\"].values:\n",
    "        accented=convertStress(other_words[other_words[\"id\"]==word_id].iloc[0][\"accented\"])\n",
    "    return accented\n",
    "\n",
    "def get_extra_info(word_id: int, Type: str):\n",
    "    info={}\n",
    "    if Type==\"noun\":\n",
    "        if word_id in nouns_csv[\"word_id\"].values:\n",
    "            row=nouns_csv[nouns_csv[\"word_id\"]==word_id].iloc[0]\n",
    "            info={\n",
    "                \"gender\": row[\"gender\"],\n",
    "                \"partner\": convertStress(row[\"partner\"]),\n",
    "                \"indeclinable\": row[\"indeclinable\"],\n",
    "                \"animate\": row[\"animate\"],\n",
    "                \"sg_only\": row[\"sg_only\"],\n",
    "                \"pl_only\": row[\"pl_only\"],\n",
    "            }\n",
    "    elif Type==\"verb\":\n",
    "        if word_id in verbs_csv[\"word_id\"].values:\n",
    "            row=verbs_csv[verbs_csv[\"word_id\"]==word_id].iloc[0]\n",
    "            info={\n",
    "                \"aspect\": row[\"aspect\"],\n",
    "                \"partner\": convertStress(row[\"partner\"]).replace(\";\", \", \")\n",
    "            }\n",
    "    \n",
    "    return info\n",
    "\n",
    "def get_translations(word_id: int):\n",
    "    translation_list=[]\n",
    "    if word_id in translations_csv[\"word_id\"].values:\n",
    "        table=translations_csv[translations_csv[\"word_id\"]==word_id]\n",
    "        for i, row in table.iterrows():\n",
    "            translation_list.append([\n",
    "                row[\"tl\"],\n",
    "                convertStress(row[\"example_ru\"]),\n",
    "                row[\"example_tl\"],\n",
    "                row[\"info\"],\n",
    "            ])\n",
    "    return translation_list\n",
    "\n",
    "def get_translation_str(word_id: int):\n",
    "    translation_list=[]\n",
    "    if word_id in translations_csv[\"word_id\"].values:\n",
    "        table=translations_csv[translations_csv[\"word_id\"]==word_id]\n",
    "        for i, row in table.iterrows():\n",
    "            translation_list.append(row[\"tl\"])\n",
    "    return \"; \".join(translation_list)\n",
    "\n",
    "def get_expressions(word_id: int, Type: str):\n",
    "    # 若查的是单词，则返回expression列表\n",
    "    if Type!=\"expression\":\n",
    "        expression_list=[]\n",
    "        if word_id in expressions_words_csv[\"referenced_word_id\"].values:\n",
    "            expression_id_list=expressions_words_csv[expressions_words_csv[\"referenced_word_id\"]==word_id][\"expression_id\"].values.tolist()\n",
    "            for expression_id in expression_id_list:\n",
    "                expression_list.append([\n",
    "                    get_accented(expression_id),\n",
    "                    get_translation_str(expression_id)\n",
    "                ])\n",
    "        return expression_list\n",
    "    # 若查的是expression，返回单词的列表\n",
    "    else:\n",
    "        part_list=[]\n",
    "        if word_id in expressions_words_csv[\"expression_id\"].values:\n",
    "            part_id_list=expressions_words_csv[expressions_words_csv[\"expression_id\"]==word_id][\"referenced_word_id\"].values.tolist()\n",
    "            for part_id in part_id_list:\n",
    "                part_list.append([\n",
    "                    get_accented(part_id),\n",
    "                    get_translation_str(part_id)\n",
    "                ])\n",
    "        return part_list\n",
    "\n",
    "def get_sentences(word_id: int):\n",
    "    sentence_list=[]\n",
    "    if word_id in sentences_words_csv[\"word_id\"].values:\n",
    "        sentence_id_list=sentences_words_csv[sentences_words_csv[\"word_id\"]==word_id][\"sentence_id\"].values\n",
    "        for sentence_id in sentence_id_list:\n",
    "            if sentence_id in sentences_csv[\"id\"].values and sentence_id in sentences_translations_csv[\"sentence_id\"].values:\n",
    "                sentence_list.append([\n",
    "                    convertStress(sentences_csv[sentences_csv[\"id\"]==sentence_id].iloc[0][\"ru\"]),\n",
    "                    sentences_translations_csv[sentences_translations_csv[\"sentence_id\"]==sentence_id].iloc[0][\"tl_en\"],\n",
    "                ])\n",
    "            # 取前10个\n",
    "            if len(sentence_list)==10:\n",
    "                break\n",
    "    return sentence_list\n",
    "\n",
    "def get_forms(word_id: int):\n",
    "    forms_dict={}\n",
    "    if word_id in words_forms_csv[\"word_id\"].values:\n",
    "        table=words_forms_csv[words_forms_csv[\"word_id\"]==word_id]\n",
    "        forms_list=table[\"form_type\"].unique().tolist()\n",
    "        for form_type in forms_list:\n",
    "            form=\", \".join(table[table[\"form_type\"]==form_type][\"form\"])\n",
    "            # form_bare=\", \".join(table[table[\"form_type\"]==form_type][\"form_bare\"])\n",
    "            forms_dict[form_type]=convertStress(form)\n",
    "    return forms_dict\n",
    "\n",
    "def get_relateds(word_id: int):\n",
    "    relateds_word={\n",
    "        \"related\":[],\n",
    "        \"synonym\":[],\n",
    "        \"antonym\":[]\n",
    "    }\n",
    "    if word_id in words_rels_csv[\"word_id\"].values:\n",
    "        table=words_rels_csv[words_rels_csv[\"word_id\"]==word_id]\n",
    "        for i, row in table.iterrows():\n",
    "            rel_word_id=row[\"rel_word_id\"]\n",
    "            relation=row[\"relation\"]\n",
    "            relateds_word[relation].append(rel_word_id)\n",
    "    if word_id in words_rels_csv[\"rel_word_id\"].values:\n",
    "        table=words_rels_csv[words_rels_csv[\"rel_word_id\"]==word_id]\n",
    "        for i, row in table.iterrows():\n",
    "            rel_word_id=row[\"word_id\"]\n",
    "            relation=row[\"relation\"]\n",
    "            if rel_word_id not in relateds_word[relation]:\n",
    "                relateds_word[relation].append(rel_word_id)\n",
    "    for k in relateds_word:\n",
    "        relateds_word[k] = [ [get_accented(v), get_translation_str(v)] for v in relateds_word[k] ]\n",
    "    return relateds_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={}\n",
    "print(len(selected_words))\n",
    "for i,row in tqdm(selected_words.iterrows()):\n",
    "    word_id=row[\"id\"]\n",
    "    bare=row[\"bare\"]\n",
    "    accented = row[\"accented\"]\n",
    "    derived_from_word_id = row[\"derived_from_word_id\"]\n",
    "    rank = row[\"rank\"]\n",
    "    usage_en = row[\"usage_en\"]\n",
    "    Type=row[\"type\"]\n",
    "    \n",
    "    if word_dict.get(bare)==None:\n",
    "        word_dict[bare]=[]\n",
    "    \n",
    "    temp_dict={\n",
    "        \"id\": word_id,\n",
    "        \"overview\":{\n",
    "            \"type\": Type,\n",
    "            \"accented\": convertStress(accented),\n",
    "            \"derived_from_word\": get_accented(derived_from_word_id),\n",
    "            \"rank\": rank\n",
    "        },\n",
    "        \"extra\": get_extra_info(word_id, Type),\n",
    "        \"translations\": get_translations(word_id),\n",
    "        \"usage\": usage_en,\n",
    "        \"expressions\": get_expressions(word_id, Type),\n",
    "        \"sentences\": get_sentences(word_id),\n",
    "        \"forms\": get_forms(word_id),\n",
    "        \"relateds\": get_relateds(word_id),\n",
    "    }\n",
    "    \n",
    "    word_dict[bare].append(temp_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomJSONizer(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        return bool(obj) \\\n",
    "            if isinstance(obj, np.bool_) \\\n",
    "            else super().default(obj)\n",
    "\n",
    "with open(\"dict.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_dict, f, ensure_ascii=False, cls=CustomJSONizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
