{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import *\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def show_na_column(df):\n",
    "    print(\"NaN:\", [i for i in list(df.isnull().sum().items()) if i[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=pd.read_csv(\n",
    "    \"russian3/russian3 - words.csv\",\n",
    "    usecols=[\"id\", \"bare\", \"accented\", \"derived_from_word_id\", \"rank\", \"disabled\", \"usage_en\", \"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有些词竟然还有多余的空格……\n",
    "print(words[\"bare\"].str.contains(\" $\").sum())\n",
    "print(words[\"bare\"].str.contains(\"^ \").sum())\n",
    "words[\"bare\"]=words[\"bare\"].apply(lambda x:x.strip())\n",
    "print(words[\"bare\"].str.contains(\" $\").sum())\n",
    "print(words[\"bare\"].str.contains(\"^ \").sum())\n",
    "\n",
    "print(words[\"accented\"].str.contains(\" $\").sum())\n",
    "print(words[\"accented\"].str.contains(\"^ \").sum())\n",
    "words[\"accented\"]=words[\"accented\"].apply(lambda x:x.strip())\n",
    "print(words[\"accented\"].str.contains(\" $\").sum())\n",
    "print(words[\"accented\"].str.contains(\"^ \").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"derived_from_word_id\"].fillna(-1, inplace=True)\n",
    "words[\"rank\"].fillna(-1, inplace=True)\n",
    "words[\"accented\"]=words[\"accented\"].map(convertStress)\n",
    "words[\"usage_en\"].fillna(\"\", inplace=True)\n",
    "words[\"usage_en\"].replace(\"\\\\\\\\n\", \"\\\\n\", regex=True,inplace=True)\n",
    "dtype={\"id\":\"int\", \"bare\":\"string\", \"accented\":\"string\", \"derived_from_word_id\":\"int\", \"rank\":\"int\", \"disabled\":\"int\", \"usage_en\":\"string\", \"type\":\"string\"}\n",
    "words=words.astype(dtype)\n",
    "words.info()\n",
    "show_na_column(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nan_list=words[~pd.isna(words[\"type\"])]\n",
    "print(\"Total Not NaN:\", len(not_nan_list))\n",
    "print(\"Total Not NaN (not disabled):\", len(not_nan_list[not_nan_list[\"disabled\"]==0]))\n",
    "print(\"Total Not NaN (disabled):\", len(not_nan_list[not_nan_list[\"disabled\"]==1]))\n",
    "not_nan_list=not_nan_list[not_nan_list[\"disabled\"]==1]\n",
    "print(\"Has Usage (disabled):\", len(not_nan_list[not_nan_list[\"usage_en\"].isna()==False]))\n",
    "del not_nan_list\n",
    "\n",
    "print()\n",
    "nan_list=words[pd.isna(words[\"type\"])]\n",
    "print(\"Total NaN:\", len(nan_list))\n",
    "print(\"Total NaN (not disabled):\", len(nan_list[nan_list[\"disabled\"]==0]))\n",
    "print(\"Total NaN (disabled):\", len(nan_list[nan_list[\"disabled\"]==1]))\n",
    "nan_list=nan_list[nan_list[\"disabled\"]==0]\n",
    "print(\"Has Usage (not disabled):\", len(nan_list[nan_list[\"usage_en\"].isna()==False]))\n",
    "del nan_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabled的词、type为NaN的词，将没有主页面，但是可以被relate到\n",
    "selected_words=words[~pd.isna(words[\"type\"])].copy(deep=True)\n",
    "selected_words=selected_words[selected_words[\"disabled\"]==0]\n",
    "selected_words.drop(columns=[\"disabled\"], inplace=True)\n",
    "selected_words.info()\n",
    "show_na_column(selected_words)\n",
    "selected_words_dict=selected_words.set_index(\"id\").to_dict(\"index\")\n",
    "del selected_words\n",
    "\n",
    "other_words=words[(pd.isna(words[\"type\"])) | (words[\"disabled\"]==1)].copy(deep=True)\n",
    "other_words.drop(columns=[\"bare\", \"derived_from_word_id\", \"rank\", \"disabled\", \"usage_en\", \"type\"], inplace=True)\n",
    "other_words.info()\n",
    "show_na_column(other_words)\n",
    "other_words_dict=other_words.set_index(\"id\").to_dict(\"index\")\n",
    "del other_words\n",
    "\n",
    "del words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_forms_csv=pd.read_csv(\"russian3/russian3 - words_forms.csv\", usecols=[\"word_id\",\"form_type\",\"form\"])\n",
    "words_forms_csv[\"form\"].fillna(\"\", inplace=True)\n",
    "\n",
    "# 有些词竟然还有多余的空格……\n",
    "print(words_forms_csv[\"form\"].str.contains(\" $\").sum())\n",
    "print(words_forms_csv[\"form\"].str.contains(\"^ \").sum())\n",
    "words_forms_csv[\"form\"]=words_forms_csv[\"form\"].apply(lambda x:x.strip())\n",
    "print(words_forms_csv[\"form\"].str.contains(\" $\").sum())\n",
    "print(words_forms_csv[\"form\"].str.contains(\"^ \").sum())\n",
    "# 有些词竟然还有多余的括号……\n",
    "print(words_forms_csv[\"form\"].str.contains(\"\\)\").sum())\n",
    "print(words_forms_csv[\"form\"].str.contains(\"\\(\").sum())\n",
    "words_forms_csv[\"form\"]=words_forms_csv[\"form\"].apply(lambda x:x.strip(\"()\"))\n",
    "print(words_forms_csv[\"form\"].str.contains(\"\\(\").sum())\n",
    "print(words_forms_csv[\"form\"].str.contains(\"\\)\").sum())\n",
    "\n",
    "words_forms_csv[\"form\"]=words_forms_csv[\"form\"].map(convertStress)\n",
    "dtype={\"word_id\":\"int\", \"form_type\":\"string\", \"form\":\"string\"}\n",
    "words_forms_csv=words_forms_csv.astype(dtype)\n",
    "words_forms_csv.info(show_counts=True)\n",
    "show_na_column(words_forms_csv)\n",
    "\n",
    "print(\"Builing Word Form Dict...\")\n",
    "words_forms_csv_dict={}\n",
    "print(len(words_forms_csv))\n",
    "for i, row in tqdm(words_forms_csv.iterrows()):\n",
    "    word_id=row[\"word_id\"]\n",
    "    if words_forms_csv_dict.get(word_id)==None:\n",
    "        words_forms_csv_dict[word_id]={}\n",
    "    \n",
    "    form_type=row[\"form_type\"]\n",
    "    form=row[\"form\"]\n",
    "    if words_forms_csv_dict[word_id].get(form_type)==None:\n",
    "        words_forms_csv_dict[word_id][form_type]=form\n",
    "    else:\n",
    "        words_forms_csv_dict[word_id][form_type]+=\", \"+form\n",
    "\n",
    "del words_forms_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rels_csv=pd.read_csv(\"russian3/russian3 - words_rels.csv\", usecols=[\"word_id\",\"rel_word_id\",\"relation\"])\n",
    "dtype={\"word_id\":\"int\", \"rel_word_id\":\"int\", \"relation\":\"string\"}\n",
    "words_rels_csv=words_rels_csv.astype(dtype)\n",
    "words_rels_csv.info()\n",
    "show_na_column(words_rels_csv)\n",
    "\n",
    "print(\"Builing Word Relation Dict...\")\n",
    "words_rels_csv_dict={}\n",
    "print(len(words_rels_csv))\n",
    "for i, row in tqdm(words_rels_csv.iterrows()):\n",
    "    word_id=row[\"word_id\"]\n",
    "    rel_word_id=row[\"rel_word_id\"]\n",
    "    relation=row[\"relation\"]\n",
    "    \n",
    "    if words_rels_csv_dict.get(word_id)==None:\n",
    "        words_rels_csv_dict[word_id]={\n",
    "            \"related\":[],\n",
    "            \"synonym\":[],\n",
    "            \"antonym\":[]\n",
    "        }\n",
    "    if rel_word_id not in words_rels_csv_dict[word_id][relation]:\n",
    "        words_rels_csv_dict[word_id][relation].append(rel_word_id)\n",
    "\n",
    "    if words_rels_csv_dict.get(rel_word_id)==None:\n",
    "        words_rels_csv_dict[rel_word_id]={\n",
    "            \"related\":[],\n",
    "            \"synonym\":[],\n",
    "            \"antonym\":[]\n",
    "        }\n",
    "    if word_id not in words_rels_csv_dict[rel_word_id][relation]:\n",
    "        words_rels_csv_dict[rel_word_id][relation].append(word_id)\n",
    "\n",
    "del words_rels_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_csv=pd.read_csv(\"russian3/russian3 - nouns.csv\")\n",
    "# both->b\n",
    "nouns_csv[\"gender\"]=nouns_csv[\"gender\"].map({\"f\":\"f\", \"m\":\"m\", \"n\":\"n\", \"pl\":\"pl\",\"both\":\"b\"})\n",
    "nouns_csv[\"gender\"].fillna(\"\", inplace=True)\n",
    "nouns_csv[\"partner\"].fillna(\"\", inplace=True)\n",
    "nouns_csv[\"partner\"]=nouns_csv[\"partner\"].map(convertStress)\n",
    "nouns_csv[\"animate\"].fillna(0, inplace=True)\n",
    "nouns_csv[\"indeclinable\"].fillna(0, inplace=True)\n",
    "nouns_csv[\"sg_only\"].fillna(0, inplace=True)\n",
    "nouns_csv[\"pl_only\"].fillna(0, inplace=True)\n",
    "dtype={\"word_id\":\"int\", \"gender\":\"string\", \"partner\":\"string\", \"animate\":\"bool\", \"indeclinable\":\"bool\", \"sg_only\":\"bool\", \"pl_only\":\"bool\"}\n",
    "nouns_csv=nouns_csv.astype(dtype)\n",
    "nouns_csv.info()\n",
    "show_na_column(nouns_csv)\n",
    "\n",
    "nouns_csv_dict=nouns_csv.set_index(\"word_id\").to_dict(\"index\")\n",
    "del nouns_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_csv=pd.read_csv(\"russian3/russian3 - verbs.csv\", usecols=[\"word_id\",\"aspect\",\"partner\"])\n",
    "# imperfective->i, perfective->p, both->b\n",
    "verbs_csv[\"aspect\"]=verbs_csv[\"aspect\"].map({\"imperfective\":\"i\", \"perfective\":\"p\", \"both\":\"b\"})\n",
    "verbs_csv[\"aspect\"].fillna(\"\", inplace=True)\n",
    "\n",
    "def func(s):\n",
    "    return convertStress(s).replace(\";\", \", \")\n",
    "verbs_csv[\"partner\"].fillna(\"\", inplace=True)\n",
    "verbs_csv[\"partner\"]=verbs_csv[\"partner\"].map(func)\n",
    "\n",
    "dtype={\"word_id\":\"int\", \"aspect\":\"string\", \"partner\":\"string\"}\n",
    "verbs_csv=verbs_csv.astype(dtype)\n",
    "verbs_csv.info()\n",
    "show_na_column(verbs_csv)\n",
    "\n",
    "verbs_csv_dict=verbs_csv.set_index(\"word_id\").to_dict(\"index\")\n",
    "del verbs_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expressions_words_csv=pd.read_csv(\"russian3/russian3 - expressions_words.csv\", usecols=[\"expression_id\", \"referenced_word_id\"])\n",
    "dtype={\"expression_id\":\"int\", \"referenced_word_id\":\"int\"}\n",
    "expressions_words_csv=expressions_words_csv.astype(dtype)\n",
    "expressions_words_csv.info()\n",
    "show_na_column(expressions_words_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations_csv=pd.read_csv(\"russian3/russian3 - translations.csv\")\n",
    "translations_csv=translations_csv[translations_csv[\"lang\"]==\"en\"] # 只留英语的翻译\n",
    "translations_csv.drop(columns=[\"id\", \"lang\", \"position\"], inplace=True)\n",
    "translations_csv[\"example_ru\"].fillna(\"\", inplace=True)\n",
    "translations_csv[\"example_ru\"]=translations_csv[\"example_ru\"].map(convertStress)\n",
    "translations_csv[\"example_tl\"].fillna(\"\", inplace=True)\n",
    "translations_csv[\"info\"].fillna(\"\", inplace=True)\n",
    "dtype={\"word_id\":\"int\", \"tl\":\"string\", \"example_ru\":\"string\", \"example_tl\":\"string\", \"info\":\"string\"}\n",
    "translations_csv=translations_csv.astype(dtype)\n",
    "translations_csv.info()\n",
    "show_na_column(translations_csv)\n",
    "\n",
    "print(\"Builing Word Translation Dict...\")\n",
    "translations_csv_dict={}\n",
    "print(len(translations_csv))\n",
    "for i, row in tqdm(translations_csv.iterrows()):\n",
    "    word_id=row[\"word_id\"]\n",
    "    if translations_csv_dict.get(word_id)==None:\n",
    "        translations_csv_dict[word_id]=[]\n",
    "    \n",
    "    translations_csv_dict[word_id].append([\n",
    "        row[\"tl\"],\n",
    "        row[\"example_ru\"],\n",
    "        row[\"example_tl\"],\n",
    "        row[\"info\"],\n",
    "    ])\n",
    "\n",
    "del translations_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_translations_csv=pd.read_csv(\"russian3/russian3 - sentences_translations.csv\", usecols=[\"sentence_id\", \"tl_en\"])\n",
    "sentences_translations_csv=sentences_translations_csv[sentences_translations_csv[\"tl_en\"].isna()==False]\n",
    "dtype={\"sentence_id\":\"int\", \"tl_en\":\"string\"}\n",
    "sentences_translations_csv=sentences_translations_csv.astype(dtype)\n",
    "sentences_translations_csv.info()\n",
    "show_na_column(sentences_translations_csv)\n",
    "\n",
    "sentences_translations_csv_dict=sentences_translations_csv.set_index(\"sentence_id\").to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_csv=pd.read_csv(\"russian3/russian3 - sentences.csv\", usecols=[\"id\", \"ru\"])\n",
    "dtype={\"id\":\"int\", \"ru\":\"string\"}\n",
    "sentences_csv=sentences_csv.astype(dtype)\n",
    "# 剔除没有翻译的\n",
    "sentences_csv=sentences_csv[sentences_csv[\"id\"].isin(sentences_translations_csv[\"sentence_id\"])]\n",
    "sentences_csv[\"ru\"]=sentences_csv[\"ru\"].map(convertStress)\n",
    "sentences_csv.info()\n",
    "show_na_column(sentences_csv)\n",
    "\n",
    "sentences_csv_dict=sentences_csv.set_index(\"id\").to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_words_csv=pd.read_csv(\"russian3/russian3 - sentences_words.csv\", usecols=[\"sentence_id\", \"word_id\"])\n",
    "dtype={\"sentence_id\":\"int\", \"word_id\":\"int\"}\n",
    "sentences_words_csv=sentences_words_csv.astype(dtype)\n",
    "# 剔除没有翻译的\n",
    "sentences_words_csv=sentences_words_csv[sentences_words_csv[\"sentence_id\"].isin(sentences_translations_csv[\"sentence_id\"])]\n",
    "sentences_words_csv.info(show_counts=True)\n",
    "show_na_column(sentences_words_csv)\n",
    "\n",
    "print(\"Builing Sentence Dict...\")\n",
    "sentences_words_csv_dict={}\n",
    "print(len(sentences_words_csv))\n",
    "for i, row in tqdm(sentences_words_csv.iterrows()):\n",
    "    word_id=row[\"word_id\"]\n",
    "    sentence_id=row[\"sentence_id\"]\n",
    "    if sentences_words_csv_dict.get(word_id)==None:\n",
    "        sentences_words_csv_dict[word_id]=[]\n",
    "\n",
    "    # 取前10个\n",
    "    if len(sentences_words_csv_dict[word_id])<10:\n",
    "        sentences_words_csv_dict[word_id].append([\n",
    "            sentences_csv_dict[sentence_id][\"ru\"],\n",
    "            sentences_translations_csv_dict[sentence_id][\"tl_en\"],\n",
    "        ])\n",
    "\n",
    "del sentences_csv\n",
    "del sentences_csv_dict\n",
    "del sentences_words_csv\n",
    "del sentences_translations_csv\n",
    "del sentences_translations_csv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accented(word_id: int):\n",
    "    accented=\"\"\n",
    "    try:\n",
    "        accented=selected_words_dict[word_id][\"accented\"]\n",
    "    except:\n",
    "        try:\n",
    "            accented=other_words_dict[word_id][\"accented\"]\n",
    "        except:\n",
    "            pass\n",
    "    return accented\n",
    "\n",
    "def get_extra_info(word_id: int, Type: str):\n",
    "    info={}\n",
    "    if Type==\"noun\":\n",
    "        try:\n",
    "            info=nouns_csv_dict[word_id]\n",
    "        except:\n",
    "            pass\n",
    "    elif Type==\"verb\":\n",
    "        try:\n",
    "            info=verbs_csv_dict[word_id]\n",
    "        except:\n",
    "            pass\n",
    "    return info\n",
    "\n",
    "def get_translations(word_id: int):\n",
    "    translation_list=[]\n",
    "    try:\n",
    "        translation_list=translations_csv_dict[word_id]\n",
    "    except:\n",
    "        pass\n",
    "    return translation_list\n",
    "\n",
    "def get_translation_str(word_id: int):\n",
    "    translation_list=[]\n",
    "    try:\n",
    "        translation_list=[i[0] for i in translations_csv_dict[word_id]]\n",
    "    except:\n",
    "        pass\n",
    "    return \"; \".join(translation_list)\n",
    "\n",
    "def get_expressions(word_id: int, Type: str):\n",
    "    # 若查的是单词，则返回expression列表\n",
    "    if Type!=\"expression\":\n",
    "        expression_list=[]\n",
    "        if word_id in expressions_words_csv[\"referenced_word_id\"].values:\n",
    "            expression_id_list=expressions_words_csv[expressions_words_csv[\"referenced_word_id\"]==word_id][\"expression_id\"].values.tolist()\n",
    "            for expression_id in expression_id_list:\n",
    "                expression_list.append([\n",
    "                    get_accented(expression_id),\n",
    "                    get_translation_str(expression_id)\n",
    "                ])\n",
    "        return expression_list\n",
    "    # 若查的是expression，返回单词的列表\n",
    "    else:\n",
    "        part_list=[]\n",
    "        if word_id in expressions_words_csv[\"expression_id\"].values:\n",
    "            part_id_list=expressions_words_csv[expressions_words_csv[\"expression_id\"]==word_id][\"referenced_word_id\"].values.tolist()\n",
    "            for part_id in part_id_list:\n",
    "                part_list.append([\n",
    "                    get_accented(part_id),\n",
    "                    get_translation_str(part_id)\n",
    "                ])\n",
    "        return part_list\n",
    "\n",
    "def get_sentences(word_id: int):\n",
    "    sentence_list=[]\n",
    "    try:\n",
    "        sentence_list=sentences_words_csv_dict[word_id]\n",
    "    except:\n",
    "        pass\n",
    "    return sentence_list\n",
    "\n",
    "def get_forms(word_id: int):\n",
    "    forms_dict={}\n",
    "    try:\n",
    "        forms_dict=words_forms_csv_dict[word_id]\n",
    "    except:\n",
    "        pass\n",
    "    return forms_dict\n",
    "\n",
    "def get_relateds(word_id: int):\n",
    "    relateds_word={\n",
    "        \"related\":[],\n",
    "        \"synonym\":[],\n",
    "        \"antonym\":[]\n",
    "    }\n",
    "    try:\n",
    "        relateds_word=words_rels_csv_dict[word_id]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    relateds={}\n",
    "    for k in relateds_word:\n",
    "        relateds[k] = [ [get_accented(v), get_translation_str(v)] for v in relateds_word[k] ]\n",
    "    return relateds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={}\n",
    "print(len(selected_words_dict))\n",
    "\n",
    "for word_id, value in tqdm(selected_words_dict.items()):\n",
    "\n",
    "    bare=value[\"bare\"]\n",
    "    accented = value[\"accented\"]\n",
    "    derived_from_word_id = value[\"derived_from_word_id\"]\n",
    "    rank = value[\"rank\"]\n",
    "    usage_en = value[\"usage_en\"]\n",
    "    Type=value[\"type\"]\n",
    "    \n",
    "    if word_dict.get(bare)==None:\n",
    "        word_dict[bare]=[]\n",
    "    \n",
    "    temp_dict={\n",
    "        \"id\": word_id,\n",
    "        \"overview\":{\n",
    "            \"type\": Type,\n",
    "            \"accented\": accented,\n",
    "            \"derived_from_word\": get_accented(derived_from_word_id),\n",
    "            \"rank\": rank\n",
    "        },\n",
    "        \"extra\": get_extra_info(word_id, Type),\n",
    "        \"translations\": get_translations(word_id),\n",
    "        \"usage\": usage_en,\n",
    "        \"expressions\": get_expressions(word_id, Type),\n",
    "        \"sentences\": get_sentences(word_id),\n",
    "        \"forms\": get_forms(word_id),\n",
    "        \"relateds\": get_relateds(word_id),\n",
    "    }\n",
    "    \n",
    "    word_dict[bare].append(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomJSONizer(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        return bool(obj) \\\n",
    "            if isinstance(obj, np.bool_) \\\n",
    "            else super().default(obj)\n",
    "\n",
    "with open(\"dict.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_dict, f, ensure_ascii=False, cls=CustomJSONizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
